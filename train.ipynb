{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newsSubTitle'] = df['newsSubTitle'].fillna(\"부제목없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [\"Unnamed: 0.1\",\t\"Unnamed: 0\",\"processSentencenum\"]\n",
    "df = df.drop(columns=drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newsSubcategory'] = df['newsSubcategory'].fillna(\"부카테고리없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newsID             0\n",
       "newsCategory       0\n",
       "newsSubcategory    0\n",
       "newsTitle          0\n",
       "newsSubTitle       0\n",
       "newsContent        0\n",
       "partNum            0\n",
       "useType            0\n",
       "processType        0\n",
       "processPattern     0\n",
       "processLevel       0\n",
       "sentenceCount      0\n",
       "sentenceInfo       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_validation = pd.read_csv('./data/validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0.1              0\n",
       "Unnamed: 0                0\n",
       "newsID                    0\n",
       "newsCategory              0\n",
       "newsSubcategory         210\n",
       "newsTitle                 0\n",
       "newsSubTitle          45703\n",
       "newsContent               0\n",
       "partNum                   0\n",
       "useType                   0\n",
       "processType               0\n",
       "processPattern            0\n",
       "processLevel              0\n",
       "sentenceCount             0\n",
       "sentenceInfo              0\n",
       "processSentencenum    34418\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = [\"Unnamed: 0.1\",\t\"Unnamed: 0\",\"processSentencenum\"]\n",
    "df_validation = df_validation.drop(columns=drop)\n",
    "df_validation['newsSubTitle'] = df_validation['newsSubTitle'].fillna(\"부제목없음\")\n",
    "df_validation['newsSubcategory'] = df_validation['newsSubcategory'].fillna(\"부카테고리없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'train.csv'로 저장\n",
    "df.to_csv('./data/train.csv', index=False)\n",
    "\n",
    "# 'validation.csv'로 저장\n",
    "df_validation.to_csv('./validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"klue/roberta-large\" # \"klue/bert-base\", \"klue/bert-large\", \"klue/roberta-base\"\n",
    "batch_size = 64\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = load_dataset('csv', data_files='../data/train_1.csv')\n",
    "raw_test = load_dataset('csv', data_files='../data/validation_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo'],\n",
       "        num_rows: 202948\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo'],\n",
       "        num_rows: 22550\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo'],\n",
       "        num_rows: 69280\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, valid= raw_train['train'].train_test_split(test_size=0.1).values()\n",
    "review_dataset = datasets.DatasetDict({'train': train, 'valid': valid, 'test': raw_test['train']})\n",
    "review_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['미국', '노스', '##캐', '##롤', '##라이나', '##서', '또', '총격', '…', '3', '##명', '사망', ',', '4', '##명', '부상']\n",
      "['부제', '##목', '##없', '##음']\n",
      "['미국', '.', '캐나다']\n",
      "['미국', '노스', '##캐', '##롤', '##라이나', '##주', '##에서', '총격', '사건', '##이', '발생', '##해', '3', '##명', '##이', '숨지', '##고', '4', '##명', '##이', '부상', '##했', '##다고', 'CNN', '##이', '3', '##일', '(', '현지', '##시간', ')', '보도', '##했', '##다', '.', '보도', '##에', '따르', '##면', '경찰', '##은', '이날', '오전', '노스', '##캐', '##롤', '##라이나', '##주', '윌', '##밍', '##턴', '##의', '한', '가정', '##집', '##에서', '총격', '##으로', '3', '##명', '##이', '사망', '##하고', '4', '##명', '##이', '다쳤', '##다고', '밝혔', '##다', '.', '경찰', '##은', '자정', '직후', '신고', '##를', '받', '##고', '출동', '##했', '##다', '.', '이', '곳', '##에서', '##는', '파티', '##가', '벌어지', '##고', '있', '##던', '것', '##으로', '전해졌', '##다', '.', '경찰', '##은', '보도', '##자료', '##에서', '이번', '사건', '##을', '두고', '[UNK]', '\"', '총격', '##전', '(', 'g', '##un', '##f', '##ight', ')', '[UNK]', '\"', '이라', '##고', '표현', '##했', '##으나', '구체', '##적인', '내용', '##은', '밝히', '##지', '않', '##았', '##다', '.', '경찰', '##에', '따르', '##면', '이날', '오전', '기준', '아무', '##도', '이번', '사건', '##으로', '구금', '##되', '##지', '않', '##았', '##다', '.', '다만', '총격', '##을', '가한', '의혹', '##을', '받', '##는', '최소', '1', '##명', '##이', '도주', '중', '##인', '것', '##으로', '전해졌', '##다', '.', '아직', '얼마나', '많', '##은', '사람', '##들이', '총', '##을', '발포', '##했', '##는지', '등', '내용', '##도', '알려', '##지', '##지', '않', '##았', '##다', '.', '경찰', '##은', '총격', '원인', '##을', '조사', '중', '##이다', '.', '사망자', '##와', '부상자', '##의', '이름', '##도', '공개', '##되', '##지', '않', '##았', '##다', '.', '최근', '미국', '##에서', '##는', '사상자', '##가', '발생', '##한', '총격', '사건', '##이', '잇따르', '##고', '있', '##다', '.', '지난달', '16', '##일', '애틀랜타', '##에서', '##는', '한인', '4', '##명', '등', '6', '##명', '##의', '아시아', '##계', '미국인', '##이', '사망', '##한', '연쇄', '총격', '사건', '##이', '발생', '##했', '##다', '.', '지난달', '22', '##일', '##에', '##는', '콜로', '##라도', '##주', '볼', '##더', '슈퍼마켓', '##에서', '총격', '##으로', '10', '##명', '##이', '숨졌', '##다', '.', '지난달', '31', '##일', '##에', '##는', '캘리포니아주', '##의', '한', '사무실', '건물', '##에서', '총격', '##이', '발생', '##해', '어린아이', '##를', '포함', '##한', '4', '##명', '##이', '숨졌', '##다', '.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d738bdc4e644c2967471d00b88aa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de1fce5c90d431b96d2dab6199ee901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339a26f2996e4e378fabcc269d43c001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69280 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(tokenizer.tokenize(train['newsTitle'][0]))\n",
    "print(tokenizer.tokenize(train['newsSubTitle'][0]))\n",
    "print(tokenizer.tokenize(train['newsSubcategory'][0]))\n",
    "print(tokenizer.tokenize(train['newsContent'][0]))\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # 여러 컬럼을 각각 토큰화\n",
    "    title_tokens = tokenizer(example[\"newsTitle\"], truncation=True)\n",
    "    subtitle_tokens = tokenizer(example[\"newsSubTitle\"], truncation=True)\n",
    "    newsContent = tokenizer(example['newsContent'], truncation=True)\n",
    "    newsSubcategory = tokenizer(example['newsSubcategory'], truncation=True)\n",
    "    return {**title_tokens, **subtitle_tokens, **newsContent, **newsSubcategory}\n",
    "\n",
    "\n",
    "tokenized_datasets = review_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(1), 'input_ids': tensor([    0,  3634,  2267,  2616,  2200, 12865,  5028,     2]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 202948\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 22550\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum', 'useType', 'processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 69280\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns(['newsID', 'newsCategory', 'newsSubcategory', 'newsTitle', 'newsSubTitle', 'newsContent', 'partNum','processType', 'processPattern', 'processLevel', 'sentenceCount', 'sentenceInfo' ])\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].rename_column(\"useType\", \"labels\")\n",
    "tokenized_datasets['valid'] = tokenized_datasets['valid'].rename_column(\"useType\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "valid_dataloader = DataLoader(tokenized_datasets[\"valid\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], shuffle=False, batch_size=batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': torch.Size([64]),\n",
       " 'input_ids': torch.Size([64, 11]),\n",
       " 'token_type_ids': torch.Size([64, 11]),\n",
       " 'attention_mask': torch.Size([64, 11])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=6) # 편의상 6으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rirakang/virtual/envev/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3172\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler, AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 장치: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# MPS 장치가 가능하면 사용, 아니면 CPU 사용\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"사용 중인 장치: {device}\")\n",
    "\n",
    "# 모델을 해당 장치로 이동\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04715fa8ae2d4abeacc62fafde02f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f71915e623d4e9cbe09a14530f4f564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb8e38ce07e424eb2d8983652b0824e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f9de71859f4ccfa9a55cd2b0a8ca5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428fde2afcd7442e9b8c11d1b0ae2bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU inference time: 2.4115 seconds\n",
      "MPS inference time: 2.1613 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 장치 설정\n",
    "device_cpu = torch.device(\"cpu\")\n",
    "device_mps = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 더미 데이터 준비\n",
    "text = [\"This is a test sentence for performance evaluation.\"] * 1000\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# CPU에서 테스트\n",
    "model.to(device_cpu)\n",
    "inputs = {k: v.to(device_cpu) for k, v in inputs.items()}\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "cpu_time = time.time() - start_time\n",
    "\n",
    "# MPS에서 테스트\n",
    "model.to(device_mps)\n",
    "inputs = {k: v.to(device_mps) for k, v in inputs.items()}\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "mps_time = time.time() - start_time\n",
    "\n",
    "print(f\"CPU inference time: {cpu_time:.4f} seconds\")\n",
    "print(f\"MPS inference time: {mps_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19efedd20a0647f5b13d5b59f7eba764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# MPS 사용 여부 확인\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델과 데이터 로드\n",
    "model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "from tqdm.auto import tqdm\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # 모델과 토크나이저 저장\n",
    "    model.save_pretrained(f\"./result/{MODEL_NAME}/{epoch}\")\n",
    "    tokenizer.save_pretrained(f\"./result/{MODEL_NAME}/{epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: packaging>17.1 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torchmetrics) (24.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torchmetrics) (2.5.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (74.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.15.4)\n",
      "Requirement already satisfied: networkx in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rirakang/virtual/envev/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid acc: 0.7292\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "import torch\n",
    "\n",
    "# 이진 분류용 Accuracy 메트릭 초기화\n",
    "accuracy = Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "prediction_list_valid = []\n",
    "target_list_valid = []\n",
    "\n",
    "model.eval()\n",
    "for batch in valid_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu()\n",
    "    targets = batch['labels'].cpu()\n",
    "\n",
    "    # 리스트에 예측값과 타겟값 추가\n",
    "    prediction_list_valid.extend(predictions)\n",
    "    target_list_valid.extend(targets)\n",
    "\n",
    "# 예측값과 타겟값을 텐서로 변환\n",
    "preds_tensor = torch.tensor(prediction_list_valid)\n",
    "targets_tensor = torch.tensor(target_list_valid)\n",
    "\n",
    "# Accuracy 계산\n",
    "valid_acc = accuracy(preds_tensor, targets_tensor).item()\n",
    "print(f'valid acc: {valid_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
