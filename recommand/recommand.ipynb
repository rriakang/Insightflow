{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobart\n",
      "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-qdji1n4q/kobart_b538f68219b34a5a876cf78086120864\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SKT-AI/KoBART /tmp/pip-install-qdji1n4q/kobart_b538f68219b34a5a876cf78086120864\n",
      "  Resolved https://github.com/SKT-AI/KoBART to commit eec563bfccf723cae8fd0fff02d5b2b09e847516\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3 (from kobart)\n",
      "  Downloading boto3-1.35.63-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pandas in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from kobart) (2.2.1)\n",
      "Collecting pytorch-lightning==1.2.1 (from kobart)\n",
      "  Downloading pytorch_lightning-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting torch==1.7.1 (from kobart)\n",
      "  Downloading torch-1.7.1-cp39-cp39-manylinux1_x86_64.whl.metadata (23 kB)\n",
      "Collecting transformers==4.3.3 (from kobart)\n",
      "  Downloading transformers-4.3.3-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pytorch-lightning==1.2.1->kobart) (1.26.4)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pytorch-lightning==1.2.1->kobart) (1.0.0)\n",
      "Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pytorch-lightning==1.2.1->kobart) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pytorch-lightning==1.2.1->kobart) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=0.8.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (2024.2.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pytorch-lightning==1.2.1->kobart) (2.15.2)\n",
      "Requirement already satisfied: typing-extensions in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==1.7.1->kobart) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from transformers==4.3.3->kobart) (3.13.1)\n",
      "Requirement already satisfied: packaging in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from transformers==4.3.3->kobart) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from transformers==4.3.3->kobart) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from transformers==4.3.3->kobart) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.3.3->kobart)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.3.3->kobart)\n",
      "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.63 (from boto3->kobart)\n",
      "  Downloading botocore-1.35.63-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->kobart)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->kobart)\n",
      "  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pandas->kobart) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pandas->kobart) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pandas->kobart) (2024.1)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.36.0,>=1.35.63->boto3->kobart)\n",
      "  Using cached urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (3.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->kobart) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.63.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (68.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from requests->transformers==4.3.3->kobart) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from requests->transformers==4.3.3->kobart) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from requests->transformers==4.3.3->kobart) (2024.2.2)\n",
      "Requirement already satisfied: click in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from sacremoses->transformers==4.3.3->kobart) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.3.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.8.1->pytorch-lightning==1.2.1->kobart) (4.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ccl/.local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (6.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ccl/.local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.2.1->kobart) (3.2.2)\n",
      "Downloading pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m814.2/814.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-1.7.1-cp39-cp39-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.35.63-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.35.63-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n",
      "Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "Building wheels for collected packages: kobart\n",
      "  Building wheel for kobart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobart: filename=kobart-0.5.1-py3-none-any.whl size=9535 sha256=f44d87596a30a82629e1cdaa41343b9aba9a194dddd7487b0ba0628db5297716\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-z31v61sk/wheels/80/29/0d/784687ff1197f1926d891601b6fb0ca5deec524d244beaf7c6\n",
      "Successfully built kobart\n",
      "Installing collected packages: tokenizers, urllib3, torch, sacremoses, jmespath, botocore, transformers, s3transfer, boto3, pytorch-lightning, kobart\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.1\n",
      "    Uninstalling urllib3-2.2.1:\n",
      "      Successfully uninstalled urllib3-2.2.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.29.2\n",
      "    Uninstalling transformers-4.29.2:\n",
      "      Successfully uninstalled transformers-4.29.2\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 2.2.4\n",
      "    Uninstalling pytorch-lightning-2.2.4:\n",
      "      Successfully uninstalled pytorch-lightning-2.2.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-upstage 0.3.0 requires tokenizers<0.20.0,>=0.19.1, but you have tokenizers 0.10.3 which is incompatible.\n",
      "lightning 2.2.2 requires torch<4.0,>=1.13.0, but you have torch 1.7.1 which is incompatible.\n",
      "pytorch-forecasting 1.0.0 requires torch<3.0.0,>=2.0.0, but you have torch 1.7.1 which is incompatible.\n",
      "pytorch-optimizer 2.12.0 requires torch>=1.10; python_version >= \"3.8\", but you have torch 1.7.1 which is incompatible.\n",
      "torchaudio 2.2.1 requires torch==2.2.1, but you have torch 1.7.1 which is incompatible.\n",
      "torchcam 0.4.0 requires torch<3.0.0,>=2.0.0, but you have torch 1.7.1 which is incompatible.\n",
      "torchmetrics 1.3.2 requires torch>=1.10.0, but you have torch 1.7.1 which is incompatible.\n",
      "torchvision 0.17.1 requires torch==2.2.1, but you have torch 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.35.63 botocore-1.35.63 jmespath-1.0.1 kobart-0.5.1 pytorch-lightning-1.2.1 s3transfer-0.10.3 sacremoses-0.1.1 tokenizers-0.10.3 torch-1.7.1 transformers-4.3.3 urllib3-1.26.20\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Using cached numpy-1.26.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "Successfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numba>=0.57 scipy scikit-learn pillow\n",
    "\n",
    "!pip install numpy==1.26.0 --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: exceptiongroup in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.9.0\n",
      "    Uninstalling h11-0.9.0:\n",
      "      Successfully uninstalled h11-0.9.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.9.1\n",
      "    Uninstalling httpcore-0.9.1:\n",
      "      Successfully uninstalled httpcore-0.9.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.13.3\n",
      "    Uninstalling httpx-0.13.3:\n",
      "      Successfully uninstalled httpx-0.13.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "googletrans 4.0.0rc1 requires httpx==0.13.3, but you have httpx 0.27.2 which is incompatible.\n",
      "nixtlats 0.3.0 requires pydantic<2, but you have pydantic 2.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.27.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numba in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (0.60.0)\n",
      "Requirement already satisfied: scipy in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: pillow in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (11.0.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from numba) (1.26.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numba scipy scikit-learn pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 340\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U accelerate\n",
    "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BartForConditionalGeneration\n",
    "# from kobart import get_kobart_tokenizer\n",
    "\n",
    "# # KoBART 모델 및 토크나이저 로드\n",
    "# model_name = \"hyunwoongko/kobart\"\n",
    "# tokenizer = get_kobart_tokenizer()\n",
    "# model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[14025, 14525, 12005, 23091, 22294, 14441, 24213, 15247, 14058, 16127,\n",
      "         14028, 28276, 15367, 14113,  9183, 10667, 12335, 15598, 14597, 16669]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "text = \"이 글은 AI 기술이 최근 급격히 발전하고 있으며, 그 응용 가능성이 무궁무진하다는 내용입니다.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추천 제목: 선거법 270조의 '6·3·3 규정'은 선거범의 재판 기한을 명시\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트 요약 (긴 텍스트에서 중요한 부분만 남기기)\n",
    "text = \"선거법 270조의 '6·3·3 규정'은 선거범의 재판 기한을 명시하고 있지만, 훈시규정으로 해석되며 사실상 사문화된 상태다. 조희대 대법원장은 이를 강행규정으로 해석해야 한다고 강조하며, 법원에 재판 기한 준수를 요청했다.\"\n",
    "\n",
    "# 입력 텍스트 토큰화\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "\n",
    "# 제목 생성\n",
    "generated_ids = model.generate(\n",
    "    inputs['input_ids'],\n",
    "    max_length=20,   # 생성할 제목의 최대 길이\n",
    "    num_beams=3,     # Beam Search 설정\n",
    "    early_stopping=True\n",
    ")\n",
    " \n",
    "# 생성된 제목 디코딩\n",
    "title = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"추천 제목:\", title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('/mount/nas/disk02/Data/Health/Mental_Health/BERT/real/train.csv')\n",
    "# df = df.drop(columns=[\"Unnamed: 0\", \"Unnamed: 0.1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useType이 1인 행만 필터링\n",
    "df = df[df['useType'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = ['newsID','newsCategory','newsSubcategory','newsSubTitle','partNum','useType','processType','processPattern','processLevel','sentenceCount','sentenceInfo','processSentencenum']\n",
    "\n",
    "df = df.drop(columns=no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsTitle</th>\n",
       "      <th>newsContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112749</th>\n",
       "      <td>[오늘의 SR이슈] 윤리이슈 휘말린 종근당, 행복경영 선언...사이버신문고 활성화</td>\n",
       "      <td>종근당이 지난 10일 새로운 경영을 선언했다고 종근당이 11일 밝혔다.\\n직원들이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112750</th>\n",
       "      <td>[오늘의 이슈] 최태원 SK 회장의 끊임없는 도전, 기업의 사회가치 실현</td>\n",
       "      <td>최태원(맨 왼쪽) SK그룹 회장과 경영진이 지난 21일 '제1회 이천포럼'에서 석학...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112751</th>\n",
       "      <td>[따뜻한 금융][7] 은행은 태생적으로 사회적일 수 있다 ' 트리오도스 은행'</td>\n",
       "      <td>문 정부 출범 이후 재무적 이익과 함께 사회적 가치 창출을 고려하는 사회적 금융이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112752</th>\n",
       "      <td>KB국민은행, 금융기관 특성 살린 사회공헌활동 펼쳐와</td>\n",
       "      <td>KB국민은행은 금융 기관으로서의 본업에 충실한 사회 공헌 활동을 해왔다.\\n경제 약...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112753</th>\n",
       "      <td>\\\"유튜브, 알고리즘 왜곡으로 광고 수익 올렸다\\\" 前 엔지니어 의혹 제기</td>\n",
       "      <td>유튜브에서 영상 하나를 보기 시작하면 한 두시간은 금방 간다.\\n시청하고 있는 동영...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225493</th>\n",
       "      <td>경남학생창의력페스티벌 개최..초·중·고 55개 팀 220명 기량 겨뤄</td>\n",
       "      <td>경남지역 초·중·고교생들이 ‘STEAM’을 활용한 생활 속 문제 해결을 위해 기량을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225494</th>\n",
       "      <td>위안부 피해자·광복절 언급한 전효성.. 韓日네티즌 공방전 [헉스]</td>\n",
       "      <td>걸그룹 시크릿 출신 가수 전효성의 SNS에서 한국과 일본 네티즌의 설전이 벌어졌다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225495</th>\n",
       "      <td>“아직 안가셨나요”...기차로 떠나는 여유 ‘늦캉스’</td>\n",
       "      <td>코레일은 16일부터 성수기를 피해 늦은 여름휴가를 즐기려는 여행객을 위해 ‘늦캉스 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225496</th>\n",
       "      <td>정신장애인 절반 건강검진 못 받아..\\\"건강보험 재정 손실로 연결\\\"</td>\n",
       "      <td>정신장애인의 절반 이상이 건강검진을 못 받고 있어 비장애인과 비교해 심각한 의료사각...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225497</th>\n",
       "      <td>교육부 ‘시흥형 초등돌봄’ 주목…왜?</td>\n",
       "      <td>[시흥=파이낸셜뉴스 강근주 기자] 시흥형 온종일 돌봄사업에 교육부가 주목하고 사업 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            newsTitle  \\\n",
       "112749  [오늘의 SR이슈] 윤리이슈 휘말린 종근당, 행복경영 선언...사이버신문고 활성화   \n",
       "112750       [오늘의 이슈] 최태원 SK 회장의 끊임없는 도전, 기업의 사회가치 실현   \n",
       "112751    [따뜻한 금융][7] 은행은 태생적으로 사회적일 수 있다 ' 트리오도스 은행'   \n",
       "112752                  KB국민은행, 금융기관 특성 살린 사회공헌활동 펼쳐와   \n",
       "112753      \\\"유튜브, 알고리즘 왜곡으로 광고 수익 올렸다\\\" 前 엔지니어 의혹 제기   \n",
       "...                                               ...   \n",
       "225493         경남학생창의력페스티벌 개최..초·중·고 55개 팀 220명 기량 겨뤄   \n",
       "225494           위안부 피해자·광복절 언급한 전효성.. 韓日네티즌 공방전 [헉스]   \n",
       "225495                  “아직 안가셨나요”...기차로 떠나는 여유 ‘늦캉스’   \n",
       "225496         정신장애인 절반 건강검진 못 받아..\\\"건강보험 재정 손실로 연결\\\"   \n",
       "225497                           교육부 ‘시흥형 초등돌봄’ 주목…왜?   \n",
       "\n",
       "                                              newsContent  \n",
       "112749  종근당이 지난 10일 새로운 경영을 선언했다고 종근당이 11일 밝혔다.\\n직원들이 ...  \n",
       "112750  최태원(맨 왼쪽) SK그룹 회장과 경영진이 지난 21일 '제1회 이천포럼'에서 석학...  \n",
       "112751  문 정부 출범 이후 재무적 이익과 함께 사회적 가치 창출을 고려하는 사회적 금융이 ...  \n",
       "112752  KB국민은행은 금융 기관으로서의 본업에 충실한 사회 공헌 활동을 해왔다.\\n경제 약...  \n",
       "112753  유튜브에서 영상 하나를 보기 시작하면 한 두시간은 금방 간다.\\n시청하고 있는 동영...  \n",
       "...                                                   ...  \n",
       "225493  경남지역 초·중·고교생들이 ‘STEAM’을 활용한 생활 속 문제 해결을 위해 기량을...  \n",
       "225494  걸그룹 시크릿 출신 가수 전효성의 SNS에서 한국과 일본 네티즌의 설전이 벌어졌다....  \n",
       "225495  코레일은 16일부터 성수기를 피해 늦은 여름휴가를 즐기려는 여행객을 위해 ‘늦캉스 ...  \n",
       "225496  정신장애인의 절반 이상이 건강검진을 못 받고 있어 비장애인과 비교해 심각한 의료사각...  \n",
       "225497  [시흥=파이낸셜뉴스 강근주 기자] 시흥형 온종일 돌봄사업에 교육부가 주목하고 사업 ...  \n",
       "\n",
       "[112749 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['newsTitle', 'newsContent'],\n",
      "        num_rows: 112749\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "dataset_koalpaca = load_dataset(\"csv\", data_files=\"/mount/nas/disk02/Data/Health/Mental_Health/BERT/real/train.csv\")\n",
    "\n",
    "# useType이 1인 행만 필터링\n",
    "filtered_dataset = dataset_koalpaca[\"train\"].filter(lambda x: x['useType'] == 1)\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "filtered_dataset = filtered_dataset.remove_columns([\n",
    "    'Unnamed: 0', 'Unnamed: 0.1', 'newsID', 'newsCategory', 'newsSubcategory',\n",
    "    'newsSubTitle', 'partNum', 'useType', 'processType', 'processPattern',\n",
    "    'processLevel', 'sentenceCount', 'sentenceInfo', 'processSentencenum'\n",
    "])\n",
    "\n",
    "# DatasetDict 생성\n",
    "dataset = DatasetDict({\"train\": filtered_dataset})\n",
    "\n",
    "# 데이터셋 출력\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newsTitle': ['[오늘의 SR이슈] 윤리이슈 휘말린 종근당, 행복경영 선언...사이버신문고 활성화',\n",
       "  '[오늘의 이슈] 최태원 SK 회장의 끊임없는 도전, 기업의 사회가치 실현'],\n",
       " 'newsContent': ['종근당이 지난 10일 새로운 경영을 선언했다고 종근당이 11일 밝혔다.\\n직원들이 행복하게 일하는 \\'행복 경영\\'이 그것이다.\\n현재 수사가 진행 중인 이장한 회장의 운전기사 폭언 사태로 떨어진 직원들의 사기를 높이고 일류 기업으로 도약하겠다는 얘기다.\\n종근당이 말하는 행복경영의 대책은 이렇다.\\n우선 종근당 계열사에 근무하는 비정규직을 올해까지 전원 정규직화한다.\\n비정규직의 고용 안정과 처우 개선을 위해서다.\\n또한, 채용 규모도 확장한다.\\n올해 하반기 200명을 채용하고, 내년에는 420명 이상을 채용할 계획이다.\\n또한, 이 중 70% 이상을 청년으로 채용하여 청년 일자리 문제 해결에 앞장서고, 2018년 전체 임직원 중 청년 고용률을 15% 수준으로 끌어올리겠다는 계획이다.\\n지난해 전체 임직원 대비 청년 고용률은 9.3%였다.\\n채용 과정에서도 \\'블라인드 채용\\'을 도입하여 성별, 나이, 가족 관계, 학력 등에서 차별하지 않을 방침이다.\\n이어 내년부터 시행되는 최저임금 7,530원을 올해 10월부터 조기 반영할 계획이다.\\n여성 근로자를 위한 복지도 늘린다.\\n사내 어린이집을 설치·운영하여 여성 근로자의 근무 환경을 개선하고, 출퇴근 시간을 자유롭게 조정할 수 있도록 유연근무제를 벌인다.\\n이 밖에 직원의 업무 역량을 높일 수 있도록 각종 프로그램을 개발해 시행할 방침이다.\\n사내 소통도 강화한다.\\n외부 전문가와 직원으로 구성된 소통위원회를 설치하여 대표 직속으로 사이버 신문고를 운영한다.\\n기존의 사회 공헌 활동 역시 강화한다.\\n고촌재단을 통해 국내·외 대학생과 대학원생들에게 지급하는 장학금 규모를 확대하고, 여대생 전용 기숙사를 추가로 신설하여 기존에 지방 출신 대학생들에게 무상으로 제공되던 기숙사의 폭을 늘린다.\\n종근당의 행복경영이 얼마나 실효성이 있을지는 아직 모른다.\\n하지만, 사이버신문고는 눈에 띈다.\\n이 회장의 운전기사 사태를 보면 이 회장의 부족한 직원 윤리 의식과 고용인과 피고용인 사이에서 어쩔 수 없이 참고 있어야 하는 피고용인의 어려움이 여실히 드러났다.\\n또한, 사내에 이러한 어려움을 호소할 창구가 없다는 문제점도 보인다.\\n그래서 종근당은 사내신문고를 만들기로 했다.\\n사전에 이러한 문제에 대해 직접 이야기할 수 있는 사내신문고가 마련된다면 제2의 운전기사 사태를 막을 수 있는 수단이 될 것으로 보인다.\\n물론 여기에는 직원의 문제를 들었을 때 적극적으로 해결해주겠다는 회사의 의식이 전제되어야 한다.\\n종근당 관계자는 \\\\\"기업이 지속해서 성장하기 위해서는 임직원들이 마음 놓고 일할 수 있는 근로 환경을 조성하는 것이 중요하다\\\\\"라며 \\\\\"임직원들이 자신의 역량을 최대한 발휘할 수 있는 기업 문화를 창출해 궁극적으로는 일류 기업으로 도약할 수 있는 기반을 마련할 계획\\\\\"이라고 말했다.',\n",
       "  '최태원(맨 왼쪽) SK그룹 회장과 경영진이 지난 21일 \\'제1회 이천포럼\\'에서 석학들의 강연을 경청하고 있다.\\n/SK 제공 최태원 SK그룹 회장(왼쪽)이 기업의 사회적 가치를 실현하기 위해 끊임없이 도전하고 있다.\\n22일 SK에 따르면 최 회장은 \\\\\"미래에는 사회적 가치를 창출하는 게 존경과 사랑받는 기업을 만드는 원천이라고 확신한다\\\\\"라며 \\\\\"SK는 경영 평가 항목에 \\'사회적 가치 창출\\'을 반영하고 \\'공유 인프라\\' 개념을 도입 중\\\\\"이라고 밝혔다.\\n최 회장의 발언은 지난 21일 SK가 서울 광진구 워커힐호텔에서 자신의 경영 철학 \\'딥 체인지\\'(Deep Change·사업 구조의 근본 혁신)를 주제로 개최한 이천포럼에서 이뤄졌다.\\n그는 특히 \\\\\"근육(재무적 가치)만 키우다가는 관절(사회적 가치)이 망가진다\\\\\"라며 \\\\\"SK가 사회적 가치 창출을 고민하는 변화의 선도자가 돼야 한다\\\\\"라고 강조했다.\\nSK는 이날 포럼에서 \\\\\"급변하는 경제·사회 환경 아래서 기업이 \\'서든 데스\\'(Sudden Death·급사)하지 않으려면 임원들이 최신 과학 기술 흐름을 파악하는 등 통찰력을 길러야 하고 사회적 가치도 공유한다고 판단했다\\\\\"라고 결론 내렸다.\\n이번 포럼에는 아시아인 중 처음으로 예일대 학장이 된 천명우 신경과학과 교수, 한국인 최초 블룸버그 석좌교수 하택집 존스홉킨스대 물리학과 교수, 한국인 최초 하버드대 종신교수 박홍근 화학과 교수 등이 나와 의견을 밝혔다.\\n최 회장은 이날 포럼 외에도 사회적 가치를 실현하기 위한 다양한 활동을 하고 있다.\\n그는 우선 지난 5월 27일 중국 상하이(上海)에서 열린 \\'2017상하이포럼\\'에서 경영 성과 평가에 사회 문제 해결에 대한 활동과 기여도를 반영하기로 했다.\\nSK는 SK이노베이션과 SK텔레콤, SK하이닉스 등 주요 계열사의 경영 성과 평가에 사회적 가치 기여도를 반영하는 핵심성과지표(KPI)를 만들고 있다.\\n이 지표는 SK가 시행 중인 사회적기업의 사회 성과 인센티브(SPC)와 유사한 내용이 될 것으로 보인다 최 회장은 각종 인프라와 경영 노하우 등 유·무형 회사 자산 170조 원을 사회와 공유하는 방안도 추진 중이다.\\n최 회장은 6월 19일 \\'2017 확대경영회의에서 \\\\\"SK가 보유한 자산 가운데 어떤 것들이 앞으로 공유 인프라로 활용될 수 있는지 고민해 달라\\\\\"라고 당부했다. 이에 SK는 7월 30일 \\'공유 인프라\\'의 세부 추진 방안을 마련하기 위해 전 계열사 임원이 한자리에 회동하는 태스크포스(TF)팀을 가동했다.\\n최 회장은 2010년부터 사회적기업사업단 만들어 사회적기업을 속속 설립하고 있다.\\n그는 사회적기업 생태계 조성을 위해 2015년 사회성과인센티브제도(SPC)를 도입해 사회적기업에 인센티브를 줬다.\\n최태원의 사회적 가치 추구 일지']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df_koalpaca = pd.DataFrame(dataset['train'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 중복 제거\n",
    "df_koalpaca = df_koalpaca.drop_duplicates(keep='first', ignore_index=True)\n",
    "\n",
    "# HuggingFace Dataset 형태로 변환\n",
    "dataset_koalpaca = Dataset.from_pandas(df_koalpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_koalpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 7624\n",
      "Requirement already satisfied: pip in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (24.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n",
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 178\n",
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchvision==0.15.2\n",
      "  Downloading torchvision-0.15.2-cp39-cp39-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Collecting torchaudio==2.0.1\n",
      "  Downloading torchaudio-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: filelock in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1)\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: numpy in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torchvision==0.15.2) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torchvision==0.15.2) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages (from torchvision==0.15.2) (10.2.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install torch==2.0.1, torchaudio==2.0.1 and torchvision==0.15.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested torch==2.0.1\n",
      "    torchvision 0.15.2 depends on torch==2.0.1\n",
      "    torchaudio 2.0.1 depends on torch==2.0.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TextStreamer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m BASE_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myanolja/EEVE-Korean-10.8B-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NF4 양자화를 위한 설정\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nf4_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m(\n\u001b[1;32m      5\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# 모델을 4비트 정밀도로 로드\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# 4비트 NormalFloat 양자화: 양자화된 파라미터의 분포 범위를 정규분포 내로 억제하여 정밀도 저하 방지\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;66;03m# 이중 양자화: 양자화를 적용하는 정수에 대해서도 양자화 적용\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16 \u001b[38;5;66;03m# 연산 속도를 높이기 위해 사용 (default: torch.float32)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     12\u001b[0m     BASE_MODEL,\n\u001b[1;32m     13\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mnf4_config,\n\u001b[1;32m     14\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"yanolja/EEVE-Korean-10.8B-v1.0\"\n",
    "\n",
    "# NF4 양자화를 위한 설정\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 모델을 4비트 정밀도로 로드\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4비트 NormalFloat 양자화: 양자화된 파라미터의 분포 범위를 정규분포 내로 억제하여 정밀도 저하 방지\n",
    "    bnb_4bit_use_double_quant=True, # 이중 양자화: 양자화를 적용하는 정수에 대해서도 양자화 적용\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # 연산 속도를 높이기 위해 사용 (default: torch.float32)\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(40960, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-47): 48 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=40960, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_input_template = \"\"\"아래는 작업을 설명하는 지시사항과 추가 정보를 제공하는 입력이 짝으로 구성됩니다. 이에 대한 적절한 응답을 작성해주세요.\n",
    "\n",
    "### 뉴스 본문 :\n",
    "{newsContent}\n",
    "\n",
    "### 입력:\n",
    "{input}\n",
    "\n",
    "### 응답:\"\"\"\n",
    "\n",
    "\n",
    "prompt_no_input_template = \"\"\"아래는 작업을 설명하는 지시사항입니다. 이에 대한 적절한 응답을 작성해주세요.\n",
    "\n",
    "### 뉴스본문:\n",
    "{newsContent}\n",
    "\n",
    "### 응답:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(news_content, user_input=None):\n",
    "    if user_input:\n",
    "        return prompt_input_template.format(newsContent=news_content, input=user_input)\n",
    "    else:\n",
    "        return prompt_no_input_template.format(newsContent=news_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title(news_content):\n",
    "    # 프롬프트 생성\n",
    "    prompt = generate_prompt(news_content)\n",
    "\n",
    "    # 입력을 토큰화\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 제목 생성\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=64,  # 생성할 제목의 최대 길이\n",
    "        num_return_sequences=1,  # 생성할 제목 개수\n",
    "        temperature=0.7,  # 다양성 조절 (0.7~1.0 추천)\n",
    "        top_p=0.9,  # 상위 90% 확률의 단어들만 샘플링\n",
    "        do_sample=True  # 샘플링 활성화\n",
    "    )\n",
    "\n",
    "    # 생성된 제목 디코딩\n",
    "    generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qlora 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,728,640 || all params: 10,820,653,056 || trainable%: 0.1454\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, # LoRA 가중치 행렬의 rank. 정수형이며 값이 작을수록 trainable parameter가 적어짐\n",
    "    lora_alpha=8, # LoRA 스케일링 팩터. 추론 시 PLM weight와 합칠 때 LoRA weight의 스케일을 일정하게 유지하기 위해 사용\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], # LoRA를 적용할 layer. 모델 아키텍처에 따라 달라짐\n",
    "    bias='none', # bias 파라미터를 학습시킬지 지정. ['none', 'all', 'lora_only']\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# 양자화된 모델을 학습하기 전, 전처리를 위해 호출\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# LoRA 학습을 위해서는 아래와 같이 peft를 사용하여 모델을 wrapping 해주어야 함\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 학습 파라미터 확인\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator 역할\n",
    "# 각 입력 시퀀스의 input_ids(토큰) 길이를 계산하고, 가장 긴 길이를 기준으로 길이가 짧은 시퀀스에는 패딩 토큰 추가\n",
    "def collate_fn(examples):\n",
    "    examples_batch = tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "    examples_batch['labels'] = examples_batch['input_ids'] # 모델 학습 평가를 위한 loss 계산을 위해 입력 토큰을 레이블로 사용\n",
    "    return examples_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_tokenized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m train_args \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainingArguments(\n\u001b[1;32m      2\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# 각 디바이스당 배치 사이즈. 작을수록(1~2) 좀 더 빠르게 alignment 됨\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m \u001b[38;5;66;03m# 저장할 체크포인트의 최대 수\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m---> 17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mdataset_tokenized\u001b[49m,\n\u001b[1;32m     18\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, \u001b[38;5;66;03m# 최대 시퀀스 길이\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[1;32m     20\u001b[0m     dataset_text_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn\n\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_tokenized' is not defined"
     ]
    }
   ],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=2, # 각 디바이스당 배치 사이즈. 작을수록(1~2) 좀 더 빠르게 alignment 됨\n",
    "    gradient_accumulation_steps=4, \n",
    "    warmup_steps=1,\n",
    "    #num_train_epochs=1,\n",
    "    max_steps=1000, \n",
    "    learning_rate=2e-4, # 학습률\n",
    "    bf16=True, # bf16 사용 (지원되는 하드웨어 확인 필요)\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\", # 8비트 AdamW 옵티마이저\n",
    "    logging_steps=50, # 로깅 빈도\n",
    "    save_total_limit=3 # 저장할 체크포인트의 최대 수\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_tokenized,\n",
    "    max_seq_length=512, # 최대 시퀀스 길이\n",
    "    args=train_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-11-18 10:13:51.848739: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-18 10:13:52.046796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-18 10:13:52.046898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-18 10:13:52.071853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 10:13:52.129508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 10:13:53.059928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:08<00:00,  1.70s/it]\n",
      "/home/ccl/anaconda3/envs/SAFER/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "Human: 지난해 초 연수를 위해 미국에 도착해 얼마 안 됐을 때 일이다. 마트들을 구경삼아 돌아보다 ‘알디(ALDI)’에서 한 노인을 만났다. 독일계 마트 알디는 낮은 가격에도 품질이 나쁘지 않아 미국에서 성장을 거듭하고 있었다. 내 옆에 선 그 노인은 혼잣말처럼 달라진 가격에 대해 말하기 시작했다. 얼마 전 이보다 낮은 가격이었는데, 지금 가격이 말이 되느냐는 게 요지였다. 연수 1년간 높은 물가, 그 노인은 몰랐을 고환율의 이중고를 체감할 수 있었다. 위 기사 요약에 대해 기사 제목 추천해줘\n",
      "Assistant:\n",
      "제목: \"알디에서 만난 노인의 이야기: 높은 물가와 고환율의 이중고\"\n",
      "\n",
      "요약:\n",
      "\n",
      "지난해 초, 연수를 위해 미국에 도착한 나는 마트들을 둘러보던 중 독일계 할인점 알디에서 한 노인을 만났다. 그 노인은 알디의 낮은 가격에도 불구하고 품질이 나쁘지 않아 미국에서 인기를 얻고 있는 마트였다. 노인은 혼잣말로 얼마 전보다 가격이 올랐다고 불만을 토로하며, 지금 가격이 말이 되느냐고 말했다. 이는 연수 기간 동안 높은 물가와 고환율로 인한 이중고를 겪고 있는 노인의 상황을 드러낸다.\n",
      "\n",
      "이 노인은 높은 물가와 고환율로 인해 겪는 어려움을 대변하는 인물로, 이러한 도전과제에 직면한 많은 이들의 삶을 반영한다. 알디에서 만난 노인의 이야기는 높은 물가와 고환율로 인해 겪는 어려움에 대한 인식을 높이고, 이러한 문제에 대처하기 위한 해결책을 모색하는 데 도움이 될 수 있다.\n",
      "\n",
      "기사에서는 높은 물가와 고환율로 인해 겪는 어려움에 대한 배경을 제공하고, 이러한 도전에 대처하기 위한 전\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "\n",
    "prompt_template = \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\nHuman: {prompt}\\nAssistant:\\n\"\n",
    "text = '지난해 초 연수를 위해 미국에 도착해 얼마 안 됐을 때 일이다. 마트들을 구경삼아 돌아보다 ‘알디(ALDI)’에서 한 노인을 만났다. 독일계 마트 알디는 낮은 가격에도 품질이 나쁘지 않아 미국에서 성장을 거듭하고 있었다. 내 옆에 선 그 노인은 혼잣말처럼 달라진 가격에 대해 말하기 시작했다. 얼마 전 이보다 낮은 가격이었는데, 지금 가격이 말이 되느냐는 게 요지였다. 연수 1년간 높은 물가, 그 노인은 몰랐을 고환율의 이중고를 체감할 수 있었다. 위 기사 요약에 대해 기사 제목 추천해줘'\n",
    "model_inputs = tokenizer(prompt_template.format(prompt=text), return_tensors='pt')\n",
    "\n",
    "outputs = model.generate(**model_inputs, max_new_tokens=256)\n",
    "output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 112749/112749 [00:07<00:00, 15192.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# CSV 파일 로드\n",
    "data_path = \"/mount/nas/disk02/Data/Health/Mental_Health/BERT/real/train.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df[df['useType'] == 1]\n",
    "# 필요한 컬럼만 선택\n",
    "df = df[[\"newsTitle\", \"newsContent\"]].dropna()\n",
    "# 전처리 함수 정의\n",
    "def preprocess_function(examples):\n",
    "    inputs = \"summarize: \" + examples['newsContent']\n",
    "    targets = examples['newsTitle']\n",
    "    return {\"input_text\": inputs, \"target_text\": targets}\n",
    "\n",
    "# Hugging Face Dataset으로 변환\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(preprocess_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "KETI-AIR/EEVE is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/KETI-AIR/EEVE/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1483\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    446\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n\u001b[0;32m--> 454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-673ad827-1c220b217c87557b29823ded;dcf57599-37cb-4c5f-8337-b0d141ecac1d)\n\nRepository Not Found for url: https://huggingface.co/KETI-AIR/EEVE/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BartTokenizer, BartForConditionalGeneration\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKETI-AIR/EEVE\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# EEVE 모델 경로\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBartTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2132\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2130\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2131\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2132\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.9/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: KETI-AIR/EEVE is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model_name = \"KETI-AIR/EEVE\"  # EEVE 모델 경로\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Datasets 형식으로 변환\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>newsTitle</th>\n",
       "      <th>newsContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>112749</th>\n",
       "      <td>[오늘의 SR이슈] 윤리이슈 휘말린 종근당, 행복경영 선언...사이버신문고 활성화</td>\n",
       "      <td>종근당이 지난 10일 새로운 경영을 선언했다고 종근당이 11일 밝혔다.\\n직원들이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112750</th>\n",
       "      <td>[오늘의 이슈] 최태원 SK 회장의 끊임없는 도전, 기업의 사회가치 실현</td>\n",
       "      <td>최태원(맨 왼쪽) SK그룹 회장과 경영진이 지난 21일 '제1회 이천포럼'에서 석학...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112751</th>\n",
       "      <td>[따뜻한 금융][7] 은행은 태생적으로 사회적일 수 있다 ' 트리오도스 은행'</td>\n",
       "      <td>문 정부 출범 이후 재무적 이익과 함께 사회적 가치 창출을 고려하는 사회적 금융이 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112752</th>\n",
       "      <td>KB국민은행, 금융기관 특성 살린 사회공헌활동 펼쳐와</td>\n",
       "      <td>KB국민은행은 금융 기관으로서의 본업에 충실한 사회 공헌 활동을 해왔다.\\n경제 약...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112753</th>\n",
       "      <td>\\\"유튜브, 알고리즘 왜곡으로 광고 수익 올렸다\\\" 前 엔지니어 의혹 제기</td>\n",
       "      <td>유튜브에서 영상 하나를 보기 시작하면 한 두시간은 금방 간다.\\n시청하고 있는 동영...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225493</th>\n",
       "      <td>경남학생창의력페스티벌 개최..초·중·고 55개 팀 220명 기량 겨뤄</td>\n",
       "      <td>경남지역 초·중·고교생들이 ‘STEAM’을 활용한 생활 속 문제 해결을 위해 기량을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225494</th>\n",
       "      <td>위안부 피해자·광복절 언급한 전효성.. 韓日네티즌 공방전 [헉스]</td>\n",
       "      <td>걸그룹 시크릿 출신 가수 전효성의 SNS에서 한국과 일본 네티즌의 설전이 벌어졌다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225495</th>\n",
       "      <td>“아직 안가셨나요”...기차로 떠나는 여유 ‘늦캉스’</td>\n",
       "      <td>코레일은 16일부터 성수기를 피해 늦은 여름휴가를 즐기려는 여행객을 위해 ‘늦캉스 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225496</th>\n",
       "      <td>정신장애인 절반 건강검진 못 받아..\\\"건강보험 재정 손실로 연결\\\"</td>\n",
       "      <td>정신장애인의 절반 이상이 건강검진을 못 받고 있어 비장애인과 비교해 심각한 의료사각...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225497</th>\n",
       "      <td>교육부 ‘시흥형 초등돌봄’ 주목…왜?</td>\n",
       "      <td>[시흥=파이낸셜뉴스 강근주 기자] 시흥형 온종일 돌봄사업에 교육부가 주목하고 사업 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            newsTitle  \\\n",
       "112749  [오늘의 SR이슈] 윤리이슈 휘말린 종근당, 행복경영 선언...사이버신문고 활성화   \n",
       "112750       [오늘의 이슈] 최태원 SK 회장의 끊임없는 도전, 기업의 사회가치 실현   \n",
       "112751    [따뜻한 금융][7] 은행은 태생적으로 사회적일 수 있다 ' 트리오도스 은행'   \n",
       "112752                  KB국민은행, 금융기관 특성 살린 사회공헌활동 펼쳐와   \n",
       "112753      \\\"유튜브, 알고리즘 왜곡으로 광고 수익 올렸다\\\" 前 엔지니어 의혹 제기   \n",
       "...                                               ...   \n",
       "225493         경남학생창의력페스티벌 개최..초·중·고 55개 팀 220명 기량 겨뤄   \n",
       "225494           위안부 피해자·광복절 언급한 전효성.. 韓日네티즌 공방전 [헉스]   \n",
       "225495                  “아직 안가셨나요”...기차로 떠나는 여유 ‘늦캉스’   \n",
       "225496         정신장애인 절반 건강검진 못 받아..\\\"건강보험 재정 손실로 연결\\\"   \n",
       "225497                           교육부 ‘시흥형 초등돌봄’ 주목…왜?   \n",
       "\n",
       "                                              newsContent  \n",
       "112749  종근당이 지난 10일 새로운 경영을 선언했다고 종근당이 11일 밝혔다.\\n직원들이 ...  \n",
       "112750  최태원(맨 왼쪽) SK그룹 회장과 경영진이 지난 21일 '제1회 이천포럼'에서 석학...  \n",
       "112751  문 정부 출범 이후 재무적 이익과 함께 사회적 가치 창출을 고려하는 사회적 금융이 ...  \n",
       "112752  KB국민은행은 금융 기관으로서의 본업에 충실한 사회 공헌 활동을 해왔다.\\n경제 약...  \n",
       "112753  유튜브에서 영상 하나를 보기 시작하면 한 두시간은 금방 간다.\\n시청하고 있는 동영...  \n",
       "...                                                   ...  \n",
       "225493  경남지역 초·중·고교생들이 ‘STEAM’을 활용한 생활 속 문제 해결을 위해 기량을...  \n",
       "225494  걸그룹 시크릿 출신 가수 전효성의 SNS에서 한국과 일본 네티즌의 설전이 벌어졌다....  \n",
       "225495  코레일은 16일부터 성수기를 피해 늦은 여름휴가를 즐기려는 여행객을 위해 ‘늦캉스 ...  \n",
       "225496  정신장애인의 절반 이상이 건강검진을 못 받고 있어 비장애인과 비교해 심각한 의료사각...  \n",
       "225497  [시흥=파이낸셜뉴스 강근주 기자] 시흥형 온종일 돌봄사업에 교육부가 주목하고 사업 ...  \n",
       "\n",
       "[112749 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 112749/112749 [00:07<00:00, 15467.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 생성 함수\n",
    "def create_prompt(example):\n",
    "    prompt = f\"뉴스 본문:\\n{example['newsContent']}\\n\\n제목 추천:\"\n",
    "    target = example['newsTitle']\n",
    "    return {\"input_text\": prompt, \"target_text\": target}\n",
    "\n",
    "# 프롬프트 생성 적용\n",
    "dataset = dataset.map(create_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['newsTitle', 'newsContent', '__index_level_0__', 'input_text', 'target_text'],\n",
       "    num_rows: 112749\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 112749/112749 [00:32<00:00, 3498.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-10.8B-v1.0\")\n",
    "\n",
    "# 토큰화 함수 수정\n",
    "def tokenize_function(examples):\n",
    "    # 입력 데이터 (뉴스 본문)\n",
    "    inputs = tokenizer(examples[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "    # 출력 데이터 (뉴스 제목)\n",
    "    targets = tokenizer(examples[\"target_text\"], truncation=True, padding=\"max_length\", max_length=64)  # 최대 길이 64로 설정\n",
    "\n",
    "    # 레이블에서 패딩 토큰을 -100으로 설정\n",
    "    targets[\"input_ids\"] = [\n",
    "        [-100 if token == tokenizer.pad_token_id else token for token in label]\n",
    "        for label in targets[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": targets[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# 토큰화 함수 적용\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 열 제거\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"input_text\", \"target_text\", \"newsContent\", \"newsTitle\"])\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"__index_level_0__\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 112749\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 32521, 29304, 32211, 29710, 28747, 13, 30490, 31341, 34092, 32305, 28705, 28740, 28734, 29415, 32506, 33371, 29189, 33826, 33072, 32193, 31341, 34092, 28705, 28740, 28740, 29415, 28705, 39957, 37963, 29043, 28723, 13, 30892, 29816, 32093, 39368, 36208, 464, 30094, 30357, 33371, 28742, 29015, 32614, 32027, 28723, 13, 34954, 32009, 32538, 32400, 34347, 32002, 29747, 29282, 32158, 32952, 34661, 29164, 29315, 32547, 30856, 38806, 28705, 39973, 32745, 33616, 32165, 32007, 32243, 28705, 39955, 32405, 32028, 30810, 32493, 32004, 32075, 31160, 35901, 28705, 40013, 39427, 28723, 13, 30490, 31341, 34092, 34729, 33422, 34794, 29187, 35390, 29538, 36891, 29043, 28723, 13, 29687, 29900, 32193, 31341, 30287, 32125, 30243, 32993, 33899, 32022, 32053, 29233, 31982, 34361, 32927, 32105, 32017, 29816, 32020, 31982, 30892, 29731, 32080, 28723, 13, 29859, 29233, 31982, 30892, 29187, 33948, 33606, 29844, 32241, 29687, 32049, 32941, 33565, 29043, 28723, 13, 37614, 28725, 34420, 33040, 29599, 33775, 32080, 28723, 13, 31519, 29426, 32016, 30192, 29164, 28705, 28750, 28734, 28734, 32789, 34420, 32035, 28725, 35408, 32089, 28705, 28781, 28750, 28734, 29955, 36027, 34420, 29723, 32125, 39956, 32027, 28723, 13, 37614, 28725, 32002, 32032, 28705, 28787, 28734, 28823, 36027, 33605, 32004, 34420, 32085, 33605, 35541, 32289, 33017, 29148, 32310, 29747, 29305, 29511, 28725, 28705, 28750, 28734, 28740, 28783, 31479, 32675, 39786, 29816, 32032, 33605, 33948, 35551, 28705, 28740, 28782, 28823, 33043, 32004, 28705, 39979, 29433, 31519, 29288, 33772, 32125, 39956, 32027, 28723, 13, 38573, 32675, 39786, 29816, 33404, 33605, 33948, 37976, 29538, 28705, 28774, 28723, 28770, 28823, 32077, 28723, 13, 38382, 32772, 32565, 464, 30502, 32664, 29470, 34420, 28742, 29189, 33206, 32085, 32087, 31044, 28725, 34559, 28725, 32813, 32517, 28725, 32172, 29626, 34911, 34411, 32200, 33157, 32081, 37894, 32027, 28723, 13, 32973, 32059, 32704, 33383, 32267, 32091, 29998, 30520, 30331, 28705, 28787, 28725, 28782, 28770, 28734, 32647, 32927, 28705, 28740, 28734, 30839, 32106, 32045, 29164, 34051, 29723, 32125, 39956, 32027, 28723, 13, 35115, 34723, 32543, 32314, 32307, 33024, 32688, 33319, 28723, 13, 29315, 29911, 34030, 35973, 32739, 28854, 35358, 32085, 32645, 34723, 32456, 33899, 35863, 39355, 28725, 32134, 37971, 31341, 33669, 36375, 34340, 29723, 32009, 32853, 36904, 31341, 30449, 32857, 32611, 32960, 28723, 13, 29015, 34336, 32199, 32716, 33161, 37027, 28705, 39955, 29415, 32009, 32853, 33715, 32244, 29614, 31965, 29189, 39794, 33383, 29723, 32081, 37894, 32027, 28723, 13, 29315, 29911, 34432, 29599, 33113, 32080, 28723, 13, 31132, 29775, 33706, 30275, 32199, 33123, 34902, 34432, 32431, 32727, 32739, 32085, 32379, 32199, 30573, 32004, 36077, 34760, 34167, 39765, 28723, 13, 29164, 31025, 29187, 32330, 32037, 37954, 32408, 32873, 33113, 32080, 28723, 13, 29511, 38042, 30014, 33411, 32227, 32625, 28854, 31132, 35805, 29844, 32609, 29816, 29693, 32435, 34049, 32022, 32098, 30711, 30331, 33040, 29200, 39575, 28725, 32052, 38700, 34529, 32011, 37953, 32337, 32725, 29143, 32071, 30058, 32085, 32985, 29148, 32759, 33060, 35805, 32435, 32088, 33834, 32377, 29848, 37841, 32011, 37953, 32629, 32547, 29189, 32688, 33319, 28723, 13, 30490, 31341, 33719, 33422, 34794, 29015, 33721, 32115, 31481, 32507, 32566, 32294, 32956, 37053, 29043, 28723, 13, 33056, 28725, 36077, 33764, 35294, 34497, 28705, 40530, 29043, 28723, 13, 29015, 32158, 32952, 34661, 29164, 29315, 34196, 29200, 33137, 32002, 32158, 32952, 35945]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 733, 35553, 29187, 20815, 29015, 37948, 28793, 36814, 29015, 37948, 28705, 39996, 31253, 37843, 32193, 31341, 30287, 28725, 33422, 34794, 33826, 1101, 29315, 33930, 33764, 29511, 32178, 29465, 29731]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0][\"input_ids\"])\n",
    "print(tokenized_dataset[0][\"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:07<00:00,  1.42s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 4비트 양자화 설정\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 모델 불러오기\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"yanolja/EEVE-Korean-10.8B-v1.0\",\n",
    "    quantization_config=nf4_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /home/ccl/.local/lib/python3.9/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (2.5.1)\n",
      "Requirement already satisfied: transformers in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (4.67.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from transformers->peft) (0.20.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ccl/anaconda3/envs/new_env/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,223,616 || all params: 10,815,148,032 || trainable%: 0.0945\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 수정 가능\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# 양자화된 모델 준비 (k-bit training)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 어댑터 추가\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # 학습 가능한 파라미터 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# DataParallel로 모델 래핑\n",
    "model = nn.DataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 112749/112749 [00:48<00:00, 2313.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_empty_examples(example):\n",
    "    return len(example[\"input_ids\"]) > 0 and len(example[\"labels\"]) > 0\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.filter(filter_empty_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 32521, 29304, 32211, 29710, 28747, 13, 30490, 31341, 34092, 32305, 28705, 28740, 28734, 29415, 32506, 33371, 29189, 33826, 33072, 32193, 31341, 34092, 28705, 28740, 28740, 29415, 28705, 39957, 37963, 29043, 28723, 13, 30892, 29816, 32093, 39368, 36208, 464, 30094, 30357, 33371, 28742, 29015, 32614, 32027, 28723, 13, 34954, 32009, 32538, 32400, 34347, 32002, 29747, 29282, 32158, 32952, 34661, 29164, 29315, 32547, 30856, 38806, 28705, 39973, 32745, 33616, 32165, 32007, 32243, 28705, 39955, 32405, 32028, 30810, 32493, 32004, 32075, 31160, 35901, 28705, 40013, 39427, 28723, 13, 30490, 31341, 34092, 34729, 33422, 34794, 29187, 35390, 29538, 36891, 29043, 28723, 13, 29687, 29900, 32193, 31341, 30287, 32125, 30243, 32993, 33899, 32022, 32053, 29233, 31982, 34361, 32927, 32105, 32017, 29816, 32020, 31982, 30892, 29731, 32080, 28723, 13, 29859, 29233, 31982, 30892, 29187, 33948, 33606, 29844, 32241, 29687, 32049, 32941, 33565, 29043, 28723, 13, 37614, 28725, 34420, 33040, 29599, 33775, 32080, 28723, 13, 31519, 29426, 32016, 30192, 29164, 28705, 28750, 28734, 28734, 32789, 34420, 32035, 28725, 35408, 32089, 28705, 28781, 28750, 28734, 29955, 36027, 34420, 29723, 32125, 39956, 32027, 28723, 13, 37614, 28725, 32002, 32032, 28705, 28787, 28734, 28823, 36027, 33605, 32004, 34420, 32085, 33605, 35541, 32289, 33017, 29148, 32310, 29747, 29305, 29511, 28725, 28705, 28750, 28734, 28740, 28783, 31479, 32675, 39786, 29816, 32032, 33605, 33948, 35551, 28705, 28740, 28782, 28823, 33043, 32004, 28705, 39979, 29433, 31519, 29288, 33772, 32125, 39956, 32027, 28723, 13, 38573, 32675, 39786, 29816, 33404, 33605, 33948, 37976, 29538, 28705, 28774, 28723, 28770, 28823, 32077, 28723, 13, 38382, 32772, 32565, 464, 30502, 32664, 29470, 34420, 28742, 29189, 33206, 32085, 32087, 31044, 28725, 34559, 28725, 32813, 32517, 28725, 32172, 29626, 34911, 34411, 32200, 33157, 32081, 37894, 32027, 28723, 13, 32973, 32059, 32704, 33383, 32267, 32091, 29998, 30520, 30331, 28705, 28787, 28725, 28782, 28770, 28734, 32647, 32927, 28705, 28740, 28734, 30839, 32106, 32045, 29164, 34051, 29723, 32125, 39956, 32027, 28723, 13, 35115, 34723, 32543, 32314, 32307, 33024, 32688, 33319, 28723, 13, 29315, 29911, 34030, 35973, 32739, 28854, 35358, 32085, 32645, 34723, 32456, 33899, 35863, 39355, 28725, 32134, 37971, 31341, 33669, 36375, 34340, 29723, 32009, 32853, 36904, 31341, 30449, 32857, 32611, 32960, 28723, 13, 29015, 34336, 32199, 32716, 33161, 37027, 28705, 39955, 29415, 32009, 32853, 33715, 32244, 29614, 31965, 29189, 39794, 33383, 29723, 32081, 37894, 32027, 28723, 13, 29315, 29911, 34432, 29599, 33113, 32080, 28723, 13, 31132, 29775, 33706, 30275, 32199, 33123, 34902, 34432, 32431, 32727, 32739, 32085, 32379, 32199, 30573, 32004, 36077, 34760, 34167, 39765, 28723, 13, 29164, 31025, 29187, 32330, 32037, 37954, 32408, 32873, 33113, 32080, 28723, 13, 29511, 38042, 30014, 33411, 32227, 32625, 28854, 31132, 35805, 29844, 32609, 29816, 29693, 32435, 34049, 32022, 32098, 30711, 30331, 33040, 29200, 39575, 28725, 32052, 38700, 34529, 32011, 37953, 32337, 32725, 29143, 32071, 30058, 32085, 32985, 29148, 32759, 33060, 35805, 32435, 32088, 33834, 32377, 29848, 37841, 32011, 37953, 32629, 32547, 29189, 32688, 33319, 28723, 13, 30490, 31341, 33719, 33422, 34794, 29015, 33721, 32115, 31481, 32507, 32566, 32294, 32956, 37053, 29043, 28723, 13, 33056, 28725, 36077, 33764, 35294, 34497, 28705, 40530, 29043, 28723, 13, 29015, 32158, 32952, 34661, 29164, 29315, 34196, 29200, 33137, 32002, 32158, 32952, 35945], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, 733, 35553, 29187, 20815, 29015, 37948, 28793, 36814, 29015, 37948, 28705, 39996, 31253, 37843, 32193, 31341, 30287, 28725, 33422, 34794, 33826, 1101, 29315, 33930, 33764, 29511, 32178, 29465, 29731]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAFER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
